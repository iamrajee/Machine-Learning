{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing other imp. libraries from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4-tf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load/downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "imdb.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, y_train = imdb.load_data(train=True)\n",
    "x_test_text, y_test = imdb.load_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  25000\n",
      "Test-set size:   25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-set size: \", len(x_train_text))\n",
    "print(\"Test-set size:  \", len(x_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = x_train_text + x_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trapped: buried alive brings us to a resort that has just opened, and is soon to close.<br /><br />We start with a guy in gear blowing up drifts, to avoid the possibility of avalanches. somehow, that doesn't make sense. anyways, he's about to blow away one particularly big one, when he notices the resort is open. despite his best efforts, higher authority tells him his day is over.<br /><br />soon, as everyone expects, an avalanche hits.<br /><br />Look, i'm not gonna reveal any more, all i can say is this was a B-movie designed for the family channel (which i just saw it on, and the fact it had no commercials proves it's a B-movie) anyways, it's a pretty decent film, but it's partially unreal.<br /><br />firsthand, when people are buried by ice and snow, they're buried. not just traced by powder. or, what about a CD for a screwdriver? it's not possible. and finally, what i can't stress enough, is that an explosion cannot stop a avalanche, guaranteed.<br /><br />furthermore, it's worth a rental or a TV viewing, but not owning. 7/10.<br /><br />The movie is rated PG, but maybe it should have received something a little more strong. a boy nearly loses his foot in an elevator, but his leg is cut around the ankle, a guy is toasted by electricity and diesel, and in the weight room, dead people are laying around.<br /><br />enjoy.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 s, sys: 58 ms, total: 26.4 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_words is None:\n",
    "    num_words = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cameo': 2111,\n",
       " 'barkeeps': 110508,\n",
       " 'executors': 84348,\n",
       " 'closet': 4312,\n",
       " 'wheeeew': 94669,\n",
       " 'voiceless': 54193,\n",
       " 'charu': 60696,\n",
       " 'maclaughlin': 57436,\n",
       " 'muldoon': 52974,\n",
       " 'schlock': 5715,\n",
       " 'unchecked': 31823,\n",
       " 'ormsby': 71031,\n",
       " 'motherf': 40560,\n",
       " \"1970's\": 4331,\n",
       " 'touristas': 80363,\n",
       " \"'heart\": 24107,\n",
       " 'distasful': 121416,\n",
       " 'kellogg': 58259,\n",
       " \"'stellar\": 114139,\n",
       " 'yashimo': 109245,\n",
       " 'favoirite': 103650,\n",
       " 'filmschool': 49217,\n",
       " 'jumpstart': 39411,\n",
       " \"o'quinn\": 37935,\n",
       " 'ds9': 18632,\n",
       " 'gravic': 55846,\n",
       " 'avocation': 58989,\n",
       " 'fanatics': 9908,\n",
       " 'carlsen': 35602,\n",
       " 'bombastic': 13762,\n",
       " 'obstructs': 83186,\n",
       " 'hooooot': 68692,\n",
       " '914': 66162,\n",
       " 'round': 2147,\n",
       " 'appropriating': 59570,\n",
       " 'insinuates': 31798,\n",
       " 'padilla': 56440,\n",
       " 'typified': 32172,\n",
       " 'wasted': 1002,\n",
       " 'polio': 47584,\n",
       " \"'inventing\": 123119,\n",
       " 'bristish': 106914,\n",
       " 'osterlich': 42224,\n",
       " \"fag'\": 78497,\n",
       " 'meats': 60686,\n",
       " 'charities': 50802,\n",
       " 'gulpilil': 25422,\n",
       " 'kellogs': 112555,\n",
       " 'pertinency': 112596,\n",
       " 'production': 353,\n",
       " \"marriages'\": 110577,\n",
       " 'bummed': 30171,\n",
       " 'aurthur': 110660,\n",
       " 'dawkins': 26326,\n",
       " 'cowell': 31093,\n",
       " 'mendes': 9907,\n",
       " \"ace's\": 93897,\n",
       " 'epilepsy': 30746,\n",
       " 'tents': 22151,\n",
       " 'represenative': 75910,\n",
       " 'cottonmouth': 42589,\n",
       " \"dilbert's\": 89213,\n",
       " 'technological': 11746,\n",
       " 'pheonix': 41094,\n",
       " 'fled': 12258,\n",
       " 'coprophagy': 119643,\n",
       " 'chothes': 81373,\n",
       " \"'10\": 40144,\n",
       " \"'queen\": 52534,\n",
       " 'guaging': 81097,\n",
       " 'classy': 6746,\n",
       " 'manifestly': 40582,\n",
       " \"rocketeer's\": 93741,\n",
       " 'andrea': 10888,\n",
       " 'passing\\x85': 75174,\n",
       " 'mccathy': 83925,\n",
       " 'scooter': 19252,\n",
       " \"'racist'\": 74045,\n",
       " 'forest¨': 74317,\n",
       " \"colbert's\": 41267,\n",
       " 'mellerdramer': 116165,\n",
       " 'musson': 64231,\n",
       " 'jeskid': 69646,\n",
       " \"candle's\": 72458,\n",
       " 'chinatown': 13154,\n",
       " \"pocket'\": 59612,\n",
       " 'immersion': 22035,\n",
       " 'dullea': 46314,\n",
       " 'willens': 92236,\n",
       " 'loathable': 77164,\n",
       " \"'neo\": 59170,\n",
       " 'standby': 29102,\n",
       " 'hieroglyphs': 74343,\n",
       " 'romanticisation': 111745,\n",
       " 'peacock': 21807,\n",
       " 'hypocrisies': 40765,\n",
       " \"trenton's\": 69584,\n",
       " \"juran's\": 105158,\n",
       " 'dehling': 42510,\n",
       " 'memeory': 123880,\n",
       " \"'distorted'\": 85813,\n",
       " 'aaghh': 89437,\n",
       " \"term'\": 99347,\n",
       " 'mortification': 93182,\n",
       " 'geta': 123645,\n",
       " 'dismembered': 20447,\n",
       " 'shufford': 100664,\n",
       " 'hilliard': 15752,\n",
       " 'pixelation': 67677,\n",
       " 'interesing': 122174,\n",
       " \"queen's\": 16624,\n",
       " \"glenda's\": 49053,\n",
       " \"philanderer'\": 86297,\n",
       " 'disqualified': 29461,\n",
       " 'traumatic': 7860,\n",
       " 'indescribably': 18769,\n",
       " '\\x91autumn': 96438,\n",
       " 'veterinarian': 30034,\n",
       " 'surpasses': 9566,\n",
       " '§1000': 89177,\n",
       " 'azz': 101043,\n",
       " 'trollop': 55874,\n",
       " 'anticompetitive': 86270,\n",
       " 'chips': 8493,\n",
       " 'melfi': 38947,\n",
       " 'classiest': 48815,\n",
       " 'commpletely': 121415,\n",
       " 'felled': 52540,\n",
       " \"bouzaglo's\": 60865,\n",
       " \"statesman's\": 75371,\n",
       " \"'kyz\": 87182,\n",
       " 'motorized': 27522,\n",
       " 'creations': 8543,\n",
       " 'ravings': 32905,\n",
       " \"l'espoir\": 82240,\n",
       " 'wobble': 30666,\n",
       " 'milafon': 102425,\n",
       " 'confusedly': 64788,\n",
       " \"ricky's\": 30345,\n",
       " 'boston': 6030,\n",
       " 'kaufman': 8766,\n",
       " '37': 8881,\n",
       " 'moarn': 109397,\n",
       " 'snapper': 57739,\n",
       " 'atheists': 22564,\n",
       " \"kathy's\": 63094,\n",
       " 'parson': 44640,\n",
       " '“stalingrad”': 35591,\n",
       " 'ciano': 50183,\n",
       " 'nauvoo': 49739,\n",
       " 'materialist': 47831,\n",
       " \"'emeutes'\": 104746,\n",
       " 'mangiati': 113385,\n",
       " 'melodious': 28619,\n",
       " 'associação': 75084,\n",
       " 'antonik': 72054,\n",
       " 'aton': 99608,\n",
       " 'gif': 122791,\n",
       " \"'gray\": 96170,\n",
       " 'unwisely': 20663,\n",
       " 'widget': 116597,\n",
       " 'tragó': 110909,\n",
       " \"spacek's\": 112983,\n",
       " 'biopic': 6063,\n",
       " 'retreated': 38358,\n",
       " \"'homosexual\": 86238,\n",
       " \"brahms'\": 72335,\n",
       " 'libertine': 32800,\n",
       " 'juggle': 26764,\n",
       " \"wells'\": 15656,\n",
       " 'nordic': 23562,\n",
       " 'outskirts': 22969,\n",
       " 'toredano': 107020,\n",
       " 'pontins': 67456,\n",
       " 'gorewhores': 112318,\n",
       " 'denigration': 39154,\n",
       " 'unintended': 13958,\n",
       " 'grubbing': 29946,\n",
       " 'madman\\x85': 120898,\n",
       " 'wok': 45314,\n",
       " 'concerns': 3317,\n",
       " 'hypnotized': 15709,\n",
       " 'avgn': 114617,\n",
       " \"warp'\": 64626,\n",
       " \"jes'\": 119104,\n",
       " 'slated': 18457,\n",
       " 'roadside': 17900,\n",
       " 'intellectualised': 96230,\n",
       " 'bargearse': 83684,\n",
       " 'wackos': 36922,\n",
       " 'aice': 104786,\n",
       " \"fonda's\": 16334,\n",
       " 'wurth': 96585,\n",
       " 'weren': 119899,\n",
       " \"den'\": 90403,\n",
       " 'cellulite': 65781,\n",
       " 'reinvent': 23578,\n",
       " \"empress's\": 110699,\n",
       " 'unatmospheric': 112129,\n",
       " 'woodenhead': 62481,\n",
       " 'elodie': 110562,\n",
       " \"commentary's\": 99377,\n",
       " \"'ugly\": 60244,\n",
       " 'discrimination': 13899,\n",
       " 'graboids': 32744,\n",
       " 'twisty': 17594,\n",
       " \"'60\": 115495,\n",
       " 'assassino': 87814,\n",
       " 'weenie': 35024,\n",
       " 'levrings': 113020,\n",
       " \"barretto's\": 92314,\n",
       " \"scrotum's\": 106154,\n",
       " 'glaringly': 16168,\n",
       " 'eady': 121513,\n",
       " 'derivative': 5600,\n",
       " 'derring': 33280,\n",
       " 'tiefenbach': 75895,\n",
       " 'paddington': 108916,\n",
       " 'dwelled': 36703,\n",
       " 'bottome': 115050,\n",
       " 'vadrouille': 116433,\n",
       " 'moreno': 22826,\n",
       " 'harassments': 63149,\n",
       " 'cornered': 15856,\n",
       " 'tore': 14584,\n",
       " 'acrobat': 33741,\n",
       " 'master': 1199,\n",
       " 'food': 1679,\n",
       " 'hausfrau': 67196,\n",
       " 'takaya': 81575,\n",
       " 'mcgavin’s': 85700,\n",
       " 'usher': 14288,\n",
       " 'thid': 96655,\n",
       " 'slanting': 119293,\n",
       " 'excursion': 15075,\n",
       " 'carruthers': 29296,\n",
       " 'rainstorm': 30571,\n",
       " \"'angel'\": 63394,\n",
       " \"priest'\": 114971,\n",
       " 'majin': 25774,\n",
       " 'boggins': 96314,\n",
       " 'heartrenching': 75257,\n",
       " 'bakke': 43516,\n",
       " \"bowlsby's\": 100388,\n",
       " 'tiebtans': 78578,\n",
       " 'patchett': 95798,\n",
       " 'perishing': 63497,\n",
       " 'ascoyne': 113091,\n",
       " 'scooby': 4667,\n",
       " 'pressuring': 27937,\n",
       " 'eastalgia': 107704,\n",
       " 'breakthrough': 9428,\n",
       " 'sugarplums': 116430,\n",
       " 'incest': 6166,\n",
       " 'reflected': 7182,\n",
       " \"autograph'\": 83879,\n",
       " 'mortis': 52593,\n",
       " 'scoff': 20712,\n",
       " 'wise': 1626,\n",
       " 'aleya': 82552,\n",
       " 'ratzoff': 71433,\n",
       " 'légitime': 105218,\n",
       " 'albertan': 114959,\n",
       " 'mead': 89591,\n",
       " 'mpaa': 10905,\n",
       " 'jug': 41960,\n",
       " 'emmaus': 117690,\n",
       " 'aod': 52486,\n",
       " \"kher's\": 119552,\n",
       " \"route'\": 97539,\n",
       " \"bs'er\": 82579,\n",
       " 'womanly': 49188,\n",
       " 'cockroaches': 24547,\n",
       " 'hynckel': 54669,\n",
       " 'sociologically': 36610,\n",
       " 'misinterpretations': 41481,\n",
       " 'giraffe': 30659,\n",
       " 'visualness': 123671,\n",
       " \"'decline'\": 120528,\n",
       " \"bennet's\": 96823,\n",
       " 'simplicities': 122892,\n",
       " 'tagalog': 37034,\n",
       " 'kwik': 63965,\n",
       " 'houselessness': 71997,\n",
       " 'wordiness': 59896,\n",
       " \"punjabi's\": 85439,\n",
       " \"sondra's\": 48921,\n",
       " 'totalitarism': 74892,\n",
       " 'bankolé': 44176,\n",
       " 'sectarian': 114421,\n",
       " 'hodgensville': 85537,\n",
       " 'phony': 4687,\n",
       " 'pendleton': 17136,\n",
       " 'mosquitoman': 79061,\n",
       " 'larquey': 110800,\n",
       " 'chickboxer': 65742,\n",
       " 'outpost': 13915,\n",
       " \"fizzle's\": 87899,\n",
       " 'file': 7583,\n",
       " 'uncontaminated': 79173,\n",
       " 'bourgeoise': 45773,\n",
       " 'pathways': 60488,\n",
       " 'asturias': 92451,\n",
       " 'p3n1': 123233,\n",
       " 'ruining': 8215,\n",
       " 'oppenheimer': 11960,\n",
       " 'dreadcentral': 93542,\n",
       " \"'shine's'\": 97444,\n",
       " 'filters': 15357,\n",
       " 'dogpatch': 93631,\n",
       " 'juxtaposition': 13775,\n",
       " 'sovjetian': 99021,\n",
       " 'dealt': 3314,\n",
       " 'buddies': 4005,\n",
       " 'reminesces': 102057,\n",
       " 'costumes\\x97so': 92870,\n",
       " \"choi's\": 119122,\n",
       " \"suzuki's\": 53684,\n",
       " 'yasbeck': 26394,\n",
       " 'pavilion': 47241,\n",
       " 'slope': 20943,\n",
       " 'afflicts': 114994,\n",
       " 'cuff': 20891,\n",
       " 'hier': 85974,\n",
       " \"t'\": 53120,\n",
       " 'dripped': 14706,\n",
       " 'norbit': 52407,\n",
       " \"evie's\": 31084,\n",
       " 'twosome': 27716,\n",
       " 'celestial': 16459,\n",
       " \"ferreri's\": 124230,\n",
       " 'modalities': 69736,\n",
       " 'payola': 54416,\n",
       " 'stanwyck': 3340,\n",
       " 'ivars': 112846,\n",
       " 'tajikistan': 69302,\n",
       " \"'damaged\": 118369,\n",
       " \"linda's\": 33785,\n",
       " 'nipsey': 105535,\n",
       " 'beeds': 118472,\n",
       " 'atwally': 108359,\n",
       " 'aslyum': 79384,\n",
       " \"'lighter'\": 103471,\n",
       " \"cellar'\": 120633,\n",
       " \"'realist'\": 58947,\n",
       " 'noncommercial': 83602,\n",
       " 'radice': 70718,\n",
       " 'schlepped': 88573,\n",
       " 'muckraker': 82786,\n",
       " 'faustian': 43453,\n",
       " \"therapist's\": 63013,\n",
       " 'steppenwolf': 116335,\n",
       " 'conspicious': 85523,\n",
       " \"spinell's\": 93947,\n",
       " 'jamukha': 55867,\n",
       " 'blessing': 9878,\n",
       " 'carol': 3949,\n",
       " 'vukovar': 53708,\n",
       " 'splinter': 26379,\n",
       " 'overemphasizing': 116820,\n",
       " 'elbowing': 95737,\n",
       " \"larriva's\": 116685,\n",
       " \"haymes's\": 121162,\n",
       " 'womanises': 93404,\n",
       " \"'tarantula'\": 116171,\n",
       " 'pultizer': 105425,\n",
       " 'mistiming': 121398,\n",
       " 'sanford': 45163,\n",
       " 'sonya': 38324,\n",
       " 'oxidant': 119098,\n",
       " 'leprosy': 29136,\n",
       " \"'musicians'\": 117268,\n",
       " 'trongard': 86827,\n",
       " 'othewise': 85327,\n",
       " 'bustiness': 76925,\n",
       " 'nags': 43188,\n",
       " 'over': 121,\n",
       " 'tacks': 34779,\n",
       " 'jumpiness': 89183,\n",
       " 'chicory': 77524,\n",
       " 's01': 79804,\n",
       " 'harchard': 80210,\n",
       " 'nabokov': 33298,\n",
       " 'textural': 45306,\n",
       " 'loonatics': 45926,\n",
       " 'hymilayan': 84052,\n",
       " 'quenching': 106160,\n",
       " 'ohrt': 85709,\n",
       " 'ensuited': 122593,\n",
       " 'budgets': 6164,\n",
       " 'popularism': 94212,\n",
       " 'jemma': 47862,\n",
       " 'dott': 93544,\n",
       " 'distillery': 58408,\n",
       " 'undefeatable': 93502,\n",
       " 'mesh': 17202,\n",
       " 'iijima': 90894,\n",
       " 'homesteaders': 38527,\n",
       " 'virtuosic': 60158,\n",
       " \"wright's\": 21224,\n",
       " 'uncannily': 23259,\n",
       " 'hinging': 113976,\n",
       " 'olga': 20074,\n",
       " \"renyolds'\": 79070,\n",
       " 'convenient': 6328,\n",
       " 'themeparks': 107104,\n",
       " 'haigh': 52122,\n",
       " 'baloons': 81407,\n",
       " 'unsupportable': 116781,\n",
       " 'cozied': 88142,\n",
       " 'clem': 67143,\n",
       " \"toni's\": 35618,\n",
       " 'blasé': 37003,\n",
       " '6f': 106659,\n",
       " 'poplin': 114521,\n",
       " 'diversity': 8301,\n",
       " \"'beep\": 38574,\n",
       " 'documentray': 108651,\n",
       " \"'classe\": 68793,\n",
       " 'silvera': 105274,\n",
       " \"sergei's\": 106121,\n",
       " 'would': 58,\n",
       " 'effects': 286,\n",
       " 'expressionally': 77974,\n",
       " 'langian': 111599,\n",
       " 'pardner': 94260,\n",
       " 'krusty': 66114,\n",
       " 'paraphrased': 34899,\n",
       " 'charmian': 64876,\n",
       " 'joint': 8093,\n",
       " \"engaging'\": 74351,\n",
       " 'sûpâ': 114702,\n",
       " 'scopophilia': 85629,\n",
       " 'gracias': 34324,\n",
       " 'plutonium': 27682,\n",
       " 'persistent': 12330,\n",
       " 'huac': 23802,\n",
       " 'agren': 26555,\n",
       " 'umptieth': 91730,\n",
       " \"'nostradamus\": 100191,\n",
       " \"stealer's\": 55223,\n",
       " 'wrestling': 3587,\n",
       " 'crochet': 89419,\n",
       " 'benja': 77320,\n",
       " 'restored\\x85': 107606,\n",
       " 'rubberized': 92792,\n",
       " 'nevil': 47801,\n",
       " 'munchausen': 38891,\n",
       " 'memorised': 117015,\n",
       " \"'bright'\": 101546,\n",
       " 'solarbabies': 88283,\n",
       " \"optimum's\": 78629,\n",
       " 'obligations': 23869,\n",
       " 'roehrig': 70753,\n",
       " 'superannuated': 64560,\n",
       " 'shahan': 80354,\n",
       " 'hanover': 105325,\n",
       " 'poignance': 53134,\n",
       " 'preda': 57959,\n",
       " 'bombings': 23084,\n",
       " 'ut': 53558,\n",
       " 'courtenay': 15692,\n",
       " 'roost': 42039,\n",
       " 'mnnage': 108797,\n",
       " \"marshal's\": 83720,\n",
       " \"'hero's\": 93797,\n",
       " 'gittai': 121976,\n",
       " 'shrieking': 14355,\n",
       " 'gse': 86469,\n",
       " \"'complaining'\": 99184,\n",
       " 'paquin': 15778,\n",
       " 'ringtone': 67339,\n",
       " 'perd': 112884,\n",
       " 'lacerations': 31742,\n",
       " 'velazquez': 32412,\n",
       " \"bustelo'\": 79877,\n",
       " 'sclerotic': 112834,\n",
       " \"beloved's\": 52282,\n",
       " 'tay': 32209,\n",
       " 'dougan': 100697,\n",
       " 'complicatedness': 72591,\n",
       " 'cuddy': 63620,\n",
       " 'cropping': 37076,\n",
       " 'bozos': 34683,\n",
       " \"spooky'n'shuddery\": 74381,\n",
       " 'neater': 96721,\n",
       " \"knotts'\": 53056,\n",
       " 'gracious': 22797,\n",
       " 'karlen': 42406,\n",
       " 'djs': 28787,\n",
       " 'aggrieved': 41906,\n",
       " 'leiter': 31027,\n",
       " 'bombs': 5537,\n",
       " 'nymphs': 27773,\n",
       " 'finises': 119089,\n",
       " 'dedalus': 61696,\n",
       " 'drivers': 9152,\n",
       " 'deceleration': 115506,\n",
       " \"duchenne's\": 77980,\n",
       " 'beasties': 37805,\n",
       " \"ago'\": 115594,\n",
       " 'minx': 45169,\n",
       " 'accussed': 102625,\n",
       " 'controversy': 6371,\n",
       " 'gapers': 85248,\n",
       " 'farmani': 77587,\n",
       " 'magnesium': 98362,\n",
       " 'therapies': 85285,\n",
       " 'twistedness': 98631,\n",
       " 'interdependent': 72031,\n",
       " 'snobby': 13373,\n",
       " 'realising': 13887,\n",
       " 'karta': 101528,\n",
       " \"'warm\": 87890,\n",
       " 'rosen': 43842,\n",
       " 'trek': 2189,\n",
       " 'redman': 36318,\n",
       " 'tarintino': 46053,\n",
       " 'echos': 33347,\n",
       " 'market': 2463,\n",
       " 'woloszczuk': 55444,\n",
       " 'mastorakis': 122026,\n",
       " 'zuotian': 102375,\n",
       " 'connoisseurship': 77618,\n",
       " 'unqualified': 30286,\n",
       " 'ritual': 6267,\n",
       " 'gibb': 23522,\n",
       " \"molnar's\": 73809,\n",
       " 'hasso': 24749,\n",
       " 'thurroughly': 119952,\n",
       " 'stockwell': 10932,\n",
       " 'wisp': 37154,\n",
       " 'dierdre': 96947,\n",
       " 'le': 2940,\n",
       " \"halley's\": 32804,\n",
       " \"'vampyros\": 77572,\n",
       " 'günter': 74585,\n",
       " 'furnished': 23021,\n",
       " 'peado': 67850,\n",
       " 'allllllllllllllllllllllllllllll': 99180,\n",
       " 'verlac': 122373,\n",
       " 'lem': 31038,\n",
       " \"kroll's\": 74912,\n",
       " 'haddofield': 84525,\n",
       " 'sweary': 96879,\n",
       " 'narcissism': 17305,\n",
       " \"right'\": 48557,\n",
       " 'lightness': 17183,\n",
       " 'marched': 24048,\n",
       " 'rimless': 89904,\n",
       " 'bartley': 43144,\n",
       " 'pitiful': 4626,\n",
       " 'investigators': 16259,\n",
       " \"kabasinski's\": 117800,\n",
       " 'bibi': 26951,\n",
       " 'mount': 10545,\n",
       " 'humberto': 57883,\n",
       " 'bodybag': 60599,\n",
       " 'glasgow': 27553,\n",
       " 'phillip': 6743,\n",
       " \"'stylish'\": 120500,\n",
       " \"there'a\": 97073,\n",
       " 'blindingly': 24658,\n",
       " 'haruhai': 111741,\n",
       " 'conditio': 90405,\n",
       " 'seperated': 57685,\n",
       " 'garcin': 82929,\n",
       " 'aldridge': 38172,\n",
       " 'sandlot': 35595,\n",
       " 'cinevista': 75551,\n",
       " 'recanting': 104024,\n",
       " 'anguishing': 40064,\n",
       " 'siani': 65222,\n",
       " \"parks'\": 60202,\n",
       " 'chimpazee': 107329,\n",
       " 'joy': 1790,\n",
       " 'mckern': 106072,\n",
       " 'um\\x85yeah\\x85pronto': 117085,\n",
       " 'slows': 9509,\n",
       " 'kevyn': 77099,\n",
       " 'arent': 34953,\n",
       " 'mineo': 55924,\n",
       " \"wirey's\": 55531,\n",
       " 'technigolour': 99901,\n",
       " 'hives': 91196,\n",
       " 'tiny': 2163,\n",
       " 'hypnotize': 28773,\n",
       " 'halaqah': 82391,\n",
       " 'mcdemorant': 78400,\n",
       " \"mistry's\": 49263,\n",
       " 'bottoms': 13481,\n",
       " 'strawn': 103085,\n",
       " \"gracie's\": 34568,\n",
       " 'tristram': 43972,\n",
       " 'does\\x85': 60968,\n",
       " 'intonation': 23645,\n",
       " 'custard': 48443,\n",
       " 'personailties': 123579,\n",
       " 'pueblos': 85201,\n",
       " 'physco': 72925,\n",
       " 'ensure': 6686,\n",
       " 'austenlike': 111969,\n",
       " \"wes'\": 91225,\n",
       " 'cattrall': 62661,\n",
       " 'gila': 35391,\n",
       " 'brassware': 82568,\n",
       " 'royalists': 121835,\n",
       " 'exercise': 3234,\n",
       " 'midwest': 18276,\n",
       " 'slender': 22311,\n",
       " 'devotional': 73556,\n",
       " 'sensetising': 123487,\n",
       " \"citizens'\": 48648,\n",
       " '\\x85\\x85\\x85': 79194,\n",
       " 'alphabetti': 91455,\n",
       " 'verily': 56999,\n",
       " 'eggs': 7140,\n",
       " 'europien': 112626,\n",
       " 'rule': 2579,\n",
       " 'heckerling': 36282,\n",
       " 'dorsal': 82657,\n",
       " 'kippei': 74243,\n",
       " 'bites': 8716,\n",
       " 'peterson': 9942,\n",
       " 'moster': 121071,\n",
       " 'weixler': 49908,\n",
       " \"who're\": 19436,\n",
       " 'latrines': 66322,\n",
       " 'pusher': 29202,\n",
       " 'abit': 33142,\n",
       " 'dheeraj': 76647,\n",
       " \"'indie'\": 71263,\n",
       " 'movers': 51865,\n",
       " 'slanders': 70740,\n",
       " \"pax's\": 89745,\n",
       " 'depressions': 111012,\n",
       " \"sgt's\": 107500,\n",
       " 'offeecious': 70361,\n",
       " 'athlete': 13572,\n",
       " 'nietszche': 60136,\n",
       " 'bevy': 15181,\n",
       " \"losers'\": 90197,\n",
       " \"'ass\": 106291,\n",
       " 'arlook': 93655,\n",
       " 'trifling': 32605,\n",
       " 'grooving': 63901,\n",
       " \"sand'\": 31134,\n",
       " 'baad': 63769,\n",
       " 'nottingham': 27638,\n",
       " 'fastly': 88080,\n",
       " 'epyon': 59242,\n",
       " '305': 38325,\n",
       " 'dcom': 41324,\n",
       " 'sacco': 61957,\n",
       " 'quint': 38093,\n",
       " 'gottfried': 26036,\n",
       " 'trifled': 52425,\n",
       " \"challenged'\": 67083,\n",
       " \"anton's\": 50522,\n",
       " 'royality': 108343,\n",
       " \"blythe's\": 108262,\n",
       " '2hours': 52030,\n",
       " 'balloo': 108785,\n",
       " 'redemption\\x97all': 110916,\n",
       " 'bewitching': 26646,\n",
       " \"gogh'\": 79403,\n",
       " 'dichotomy': 20789,\n",
       " \"dionna's\": 97280,\n",
       " \"'grandmother'\": 78439,\n",
       " 'metulskie': 67400,\n",
       " 'jirí': 36454,\n",
       " 'harltey': 93376,\n",
       " 'imbue': 29910,\n",
       " \"'cult\": 46222,\n",
       " \"silberling's\": 77370,\n",
       " \"'creators'\": 64542,\n",
       " 'st': 3087,\n",
       " 'spearhead': 50490,\n",
       " 'gableesque': 121768,\n",
       " \"decker's\": 111077,\n",
       " \"asians'\": 88850,\n",
       " 'lobotomies': 91848,\n",
       " 'admonished': 67465,\n",
       " 'daringly': 29774,\n",
       " 'stages': 5822,\n",
       " 'bellemo': 113728,\n",
       " \"comstock's\": 90478,\n",
       " 'genesis': 12509,\n",
       " 'reiterating': 40977,\n",
       " \"'instructions\": 93815,\n",
       " \"jansen's\": 50057,\n",
       " 'ambush': 15218,\n",
       " 'golfer': 30863,\n",
       " 'folky': 85824,\n",
       " 'exploitations': 87135,\n",
       " 'hiroko': 109943,\n",
       " 'probing': 16594,\n",
       " 'ruocco': 123257,\n",
       " 'zoo': 10726,\n",
       " 'vivien': 14123,\n",
       " 'gaira': 101298,\n",
       " 'bachelorettes': 122293,\n",
       " 'jesminder': 77801,\n",
       " 'stinkfest': 89789,\n",
       " \"'undercover\": 96345,\n",
       " 'drawling': 53858,\n",
       " 'livestock': 25539,\n",
       " 'quimby': 46406,\n",
       " \"'40's\": 19442,\n",
       " 'sympathises': 39820,\n",
       " 'caramel': 35815,\n",
       " 'meester': 47363,\n",
       " 'idiotized': 84625,\n",
       " 'entwining': 44381,\n",
       " \"'caringness'\": 99185,\n",
       " 'classics': 2116,\n",
       " \"ada's\": 27229,\n",
       " 'carryover': 101494,\n",
       " 'claude': 4149,\n",
       " \"'swept\": 64701,\n",
       " 'een': 53682,\n",
       " \"'haunted\": 60165,\n",
       " 'pedigreed': 70845,\n",
       " 'knight’s': 85703,\n",
       " 'ozcan': 113615,\n",
       " 'compunction': 82046,\n",
       " 'shekhar': 18004,\n",
       " 'visits': 5081,\n",
       " 'squeed': 97377,\n",
       " 'calloni': 77377,\n",
       " 'chand': 57915,\n",
       " \"glance'\": 108944,\n",
       " 'top': 342,\n",
       " 'mattheson': 102007,\n",
       " 'saleable': 53765,\n",
       " 'cinematically': 14252,\n",
       " 'saggier': 117758,\n",
       " 'gruelingly': 96041,\n",
       " 'bremen': 81071,\n",
       " 'sly': 7113,\n",
       " 'sources': 6723,\n",
       " 'phychadelic': 96100,\n",
       " 'balcan': 51058,\n",
       " 'sutras': 103003,\n",
       " \"particles'\": 115814,\n",
       " 'giss': 88160,\n",
       " 'christiano': 67116,\n",
       " 'prolifically': 101220,\n",
       " 'reflectors': 74112,\n",
       " 'dismembering': 30775,\n",
       " 'riead': 63815,\n",
       " \"patty's\": 29956,\n",
       " 'm16': 55827,\n",
       " 'baggage': 13476,\n",
       " 'liba': 57178,\n",
       " 'taradash': 56768,\n",
       " 'personable': 15324,\n",
       " \"aborigine'\": 114553,\n",
       " 'vanessa': 6745,\n",
       " 'undersold': 98032,\n",
       " \"ghose's\": 102060,\n",
       " 'straggle': 86279,\n",
       " 'dumbfoundingly': 65289,\n",
       " 'funneled': 43876,\n",
       " 'pretender': 41437,\n",
       " 'gubra': 36529,\n",
       " 'abdomen': 29247,\n",
       " \"1998's\": 43944,\n",
       " 'stockton': 49490,\n",
       " \"'rock\": 25640,\n",
       " \"'rosanne'\": 99004,\n",
       " 'schüte': 63750,\n",
       " 'horsewhips': 96679,\n",
       " 'pointless\\x97and': 112747,\n",
       " 'distaste': 17586,\n",
       " 'concretize': 118653,\n",
       " 'brooding': 6345,\n",
       " 'dumber': 6548,\n",
       " 'tardly': 75866,\n",
       " 'newlin': 101572,\n",
       " 'salmonova': 79641,\n",
       " 'varma': 13953,\n",
       " 'segall': 92684,\n",
       " 'ronald': 5588,\n",
       " 'glamis': 70591,\n",
       " \"mulligan's\": 57639,\n",
       " 'realized': 1681,\n",
       " 'popularity': 4685,\n",
       " \"'nothing'\": 50407,\n",
       " 'mountaineering': 44765,\n",
       " 'anestetic': 119964,\n",
       " 'eventuates': 52372,\n",
       " 'widened': 44403,\n",
       " 'squashy': 63589,\n",
       " 'gtf': 96112,\n",
       " \"interpol's\": 88404,\n",
       " 'pertinence': 84620,\n",
       " 'healey': 31232,\n",
       " 'sewell': 28374,\n",
       " 'graciously': 33627,\n",
       " 'lohman': 29600,\n",
       " 'improvise': 13313,\n",
       " 'warmed': 14081,\n",
       " 'heronimo': 63582,\n",
       " 'bloggers': 54349,\n",
       " 'nickles': 72721,\n",
       " 'explosive': 6629,\n",
       " 'curvature': 88445,\n",
       " 'slimane': 120763,\n",
       " 'tuxedo': 22888,\n",
       " 'tractacus': 97075,\n",
       " \"rendall's\": 64633,\n",
       " 'sailing': 12610,\n",
       " 'rawandan': 64316,\n",
       " 'fast': 714,\n",
       " '215': 77619,\n",
       " 'l': 1839,\n",
       " 'underfed': 64698,\n",
       " 'paints': 6836,\n",
       " 'hire': 3583,\n",
       " 'brugge': 119754,\n",
       " \"id'\": 57232,\n",
       " 'ciera': 62563,\n",
       " 'inf': 102722,\n",
       " 'example': 471,\n",
       " 'implying': 16997,\n",
       " 'docos': 75626,\n",
       " 'tenuously': 53392,\n",
       " \"y'alls\": 116015,\n",
       " 'subtextually': 107074,\n",
       " 'nishina': 54835,\n",
       " 'decrease': 39729,\n",
       " 'deathline': 45844,\n",
       " 'diagnoses': 49260,\n",
       " \"macy's\": 17383,\n",
       " 'forgiven': 5985,\n",
       " 'dandies': 41099,\n",
       " \"bob's\": 20570,\n",
       " 'unplanned': 37961,\n",
       " \"stalker'\": 61746,\n",
       " 'thorne': 17776,\n",
       " 'loneley': 123989,\n",
       " 'certify': 47162,\n",
       " 'comeing': 101630,\n",
       " 'sexagenarian': 123882,\n",
       " 'temmink': 90673,\n",
       " \"'pimped\": 118743,\n",
       " 'barda': 114954,\n",
       " 'bsb': 119252,\n",
       " 'fx': 3623,\n",
       " 'url': 40786,\n",
       " 'ckland': 114058,\n",
       " 'groundwork': 31828,\n",
       " 'transporting': 16209,\n",
       " \"'throne\": 103936,\n",
       " \"truffaut's\": 26303,\n",
       " 'putnam': 34752,\n",
       " 'outlier': 122534,\n",
       " 'joanous': 117643,\n",
       " 'exley': 119305,\n",
       " 'flacco': 118490,\n",
       " \"massude's\": 65009,\n",
       " 'rationalizing': 47671,\n",
       " 'berriatua': 99872,\n",
       " 'elaborations': 114756,\n",
       " 'cid': 27104,\n",
       " 'steretyped': 93350,\n",
       " 'blindspot': 55619,\n",
       " \"'functional'\": 77194,\n",
       " 'yow': 69940,\n",
       " 'waddled': 120947,\n",
       " 'denoted': 79191,\n",
       " 'resembling': 6719,\n",
       " 'frat': 7949,\n",
       " 'dynamics': 7577,\n",
       " \"wyngarde's\": 67306,\n",
       " 'aperture': 62794,\n",
       " 'chirstie': 100426,\n",
       " 'makutsi': 54277,\n",
       " \"blow'\": 113559,\n",
       " \"'rithmetic\": 86066,\n",
       " \"interesting'\\x97and\": 101305,\n",
       " \"blankfield's\": 89308,\n",
       " 'wasson': 23511,\n",
       " 'palance': 6936,\n",
       " 'rolento': 117650,\n",
       " 'lorna': 17451,\n",
       " 'crudest': 38851,\n",
       " 'chise': 38697,\n",
       " 'bombeshells': 83639,\n",
       " 'wittenburg': 111127,\n",
       " 'accredited': 77765,\n",
       " 'defying': 13709,\n",
       " \"exploration'\": 58543,\n",
       " 'etsushi': 100435,\n",
       " 'adequtely': 98734,\n",
       " 'personalized': 39796,\n",
       " \"gimbel's\": 101234,\n",
       " 'clippie': 89191,\n",
       " 'gazongas': 76712,\n",
       " 'steerforth': 121858,\n",
       " 'decorum': 33672,\n",
       " 'pda': 122151,\n",
       " 'francoisa': 76023,\n",
       " 'bhagat': 64453,\n",
       " 'weedon': 31120,\n",
       " 'pykes': 47027,\n",
       " 'smash': 6941,\n",
       " 'engages': 11123,\n",
       " 'eggert': 21928,\n",
       " \"mayne's\": 60314,\n",
       " 'hedges': 30604,\n",
       " 'neutralizes': 118258,\n",
       " 'critiques': 19165,\n",
       " 'asbestos': 31967,\n",
       " 'illuminates': 25137,\n",
       " 'confessional': 24244,\n",
       " 'alá': 98417,\n",
       " 'yay': 13863,\n",
       " \"widow's\": 35744,\n",
       " 'clunking': 37019,\n",
       " 'yuggoslavia': 95789,\n",
       " \"she'a\": 99862,\n",
       " 'duuh': 88836,\n",
       " 'pedals': 92696,\n",
       " 'spacehog': 106468,\n",
       " 'befriend': 14313,\n",
       " 'cheeziest': 113274,\n",
       " 'grem': 93796,\n",
       " 'heroistic': 74514,\n",
       " 'verite': 21681,\n",
       " 'pacific': 5957,\n",
       " \"azumi's\": 53411,\n",
       " 'anit': 105162,\n",
       " 'roadmap': 105114,\n",
       " 'ubernerds': 74087,\n",
       " 'herinteractive': 76351,\n",
       " 'withdraws': 32657,\n",
       " 'sensationalist': 28056,\n",
       " 'wilds': 24101,\n",
       " 'bewilderedly': 85730,\n",
       " 'bossy': 21296,\n",
       " 'suare': 61325,\n",
       " 'niklas': 44066,\n",
       " 'lehrman': 97187,\n",
       " 'zaphoid': 63406,\n",
       " 'beefed': 40897,\n",
       " 'charmer': 17554,\n",
       " 'throttled': 65463,\n",
       " 'horriable': 116716,\n",
       " 'squeaky': 14100,\n",
       " 'tortue': 109460,\n",
       " \"bellhop's\": 88742,\n",
       " 'procuring': 40918,\n",
       " 'b5': 34996,\n",
       " 'telesales': 75321,\n",
       " 'plage': 97878,\n",
       " 'lifetimes': 50048,\n",
       " 'beethovan': 42628,\n",
       " \"'exploitation'\": 86506,\n",
       " 'kogenta': 55240,\n",
       " 'stratagem': 46039,\n",
       " 'wrestled': 27175,\n",
       " 'nemo': 16428,\n",
       " 'surfrider': 102034,\n",
       " 'hsss': 90925,\n",
       " 'reccoemnd': 120866,\n",
       " 'kavorkian': 122196,\n",
       " 'odette': 28403,\n",
       " 'brynes': 104414,\n",
       " 'preclude': 36359,\n",
       " 'père': 36446,\n",
       " 'rampant': 8958,\n",
       " 'barometric': 81879,\n",
       " 'placated': 55937,\n",
       " 'halsslag': 57388,\n",
       " 'rpgs': 53483,\n",
       " 'normally': 1869,\n",
       " 'luz': 45428,\n",
       " 'pleasure': 1686,\n",
       " 'kosugi': 19360,\n",
       " 'concocting': 40894,\n",
       " 'gab': 34031,\n",
       " 'dede': 106164,\n",
       " 'decore': 81124,\n",
       " \"2'11\": 92112,\n",
       " 'phalocretinism': 117568,\n",
       " 'darryn': 118626,\n",
       " \"'conventional'\": 87900,\n",
       " '50ish': 93000,\n",
       " 'impair': 67804,\n",
       " 'villager': 30632,\n",
       " 'gehrig': 49218,\n",
       " 'fatness': 48231,\n",
       " 'bloodedly': 61650,\n",
       " 'welles': 3321,\n",
       " 'lawerance': 79254,\n",
       " 'dolemite': 9347,\n",
       " 'wasting': 3147,\n",
       " \"bow'\": 77413,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trapped: buried alive brings us to a resort that has just opened, and is soon to close.<br /><br />We start with a guy in gear blowing up drifts, to avoid the possibility of avalanches. somehow, that doesn't make sense. anyways, he's about to blow away one particularly big one, when he notices the resort is open. despite his best efforts, higher authority tells him his day is over.<br /><br />soon, as everyone expects, an avalanche hits.<br /><br />Look, i'm not gonna reveal any more, all i can say is this was a B-movie designed for the family channel (which i just saw it on, and the fact it had no commercials proves it's a B-movie) anyways, it's a pretty decent film, but it's partially unreal.<br /><br />firsthand, when people are buried by ice and snow, they're buried. not just traced by powder. or, what about a CD for a screwdriver? it's not possible. and finally, what i can't stress enough, is that an explosion cannot stop a avalanche, guaranteed.<br /><br />furthermore, it's worth a rental or a TV viewing, but not owning. 7/10.<br /><br />The movie is rated PG, but maybe it should have received something a little more strong. a boy nearly loses his foot in an elevator, but his leg is cut around the ankle, a guy is toasted by electricity and diesel, and in the weight room, dead people are laying around.<br /><br />enjoy.\n",
      "[2499 3625 1126  981  176    5    3 4399   12   45   39 3014    2    6\n",
      "  526    5  500    7    7   73  375   16    3  219    8 6418 3690   53\n",
      "    5  793    1 3820    4  835   12  149   94  282 3639  237   42    5\n",
      " 2310  243   27  583  191   27   50   28 7017    1 4399    6  849  467\n",
      "   24  116 1925 2060 4679  717   87   24  254    6  121    7    7  526\n",
      "   14  304 5877   32 1903    7    7  163  145   21 2174 2517   99   51\n",
      "   29   10   67  131    6   11   13    3  492   17 2611   15    1  236\n",
      " 1204   60   10   39  210    9   20    2    1  192    9   66   54 3478\n",
      " 1658   44    3  492   17 3639   44    3  180  540   19   18   44 5719\n",
      " 5176    7    7   50   83   23 3625   31 1830    2 3015  501 3625   21\n",
      "   39   31   38   48   42    3 4287   15    3   44   21  621    2  415\n",
      "   48   10  185 4848  193    6   12   32 3971  576  542    3 5769    7\n",
      "    7 3839   44  278    3 2249   38    3  240  805   18   21  702  156\n",
      "    7    7    1   17    6 1196 2997   18  273    9  142   25 2015  137\n",
      "    3  120   51  573    3  420  800 2204   24 2294    8   32 5770   18\n",
      "   24 4086    6  598  183    1    3  219    6   31 7951    2 9498    2\n",
      "    8    1 3215  676  349   83   23 8013  183    7    7  354]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_text[1])\n",
    "print(np.array(x_train_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Truncating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.27716"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94528"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2499, 3625, 1126,  981,  176,    5,    3, 4399,   12,   45,   39,\n",
       "       3014,    2,    6,  526,    5,  500,    7,    7,   73,  375,   16,\n",
       "          3,  219,    8, 6418, 3690,   53,    5,  793,    1, 3820,    4,\n",
       "        835,   12,  149,   94,  282, 3639,  237,   42,    5, 2310,  243,\n",
       "         27,  583,  191,   27,   50,   28, 7017,    1, 4399,    6,  849,\n",
       "        467,   24,  116, 1925, 2060, 4679,  717,   87,   24,  254,    6,\n",
       "        121,    7,    7,  526,   14,  304, 5877,   32, 1903,    7,    7,\n",
       "        163,  145,   21, 2174, 2517,   99,   51,   29,   10,   67,  131,\n",
       "          6,   11,   13,    3,  492,   17, 2611,   15,    1,  236, 1204,\n",
       "         60,   10,   39,  210,    9,   20,    2,    1,  192,    9,   66,\n",
       "         54, 3478, 1658,   44,    3,  492,   17, 3639,   44,    3,  180,\n",
       "        540,   19,   18,   44, 5719, 5176,    7,    7,   50,   83,   23,\n",
       "       3625,   31, 1830,    2, 3015,  501, 3625,   21,   39,   31,   38,\n",
       "         48,   42,    3, 4287,   15,    3,   44,   21,  621,    2,  415,\n",
       "         48,   10,  185, 4848,  193,    6,   12,   32, 3971,  576,  542,\n",
       "          3, 5769,    7,    7, 3839,   44,  278,    3, 2249,   38,    3,\n",
       "        240,  805,   18,   21,  702,  156,    7,    7,    1,   17,    6,\n",
       "       1196, 2997,   18,  273,    9,  142,   25, 2015,  137,    3,  120,\n",
       "         51,  573,    3,  420,  800, 2204,   24, 2294,    8,   32, 5770,\n",
       "         18,   24, 4086,    6,  598,  183,    1,    3,  219,    6,   31,\n",
       "       7951,    2, 9498,    2,    8,    1, 3215,  676,  349,   83,   23,\n",
       "       8013,  183,    7,    7,  354])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "       2499, 3625, 1126,  981,  176,    5,    3, 4399,   12,   45,   39,\n",
       "       3014,    2,    6,  526,    5,  500,    7,    7,   73,  375,   16,\n",
       "          3,  219,    8, 6418, 3690,   53,    5,  793,    1, 3820,    4,\n",
       "        835,   12,  149,   94,  282, 3639,  237,   42,    5, 2310,  243,\n",
       "         27,  583,  191,   27,   50,   28, 7017,    1, 4399,    6,  849,\n",
       "        467,   24,  116, 1925, 2060, 4679,  717,   87,   24,  254,    6,\n",
       "        121,    7,    7,  526,   14,  304, 5877,   32, 1903,    7,    7,\n",
       "        163,  145,   21, 2174, 2517,   99,   51,   29,   10,   67,  131,\n",
       "          6,   11,   13,    3,  492,   17, 2611,   15,    1,  236, 1204,\n",
       "         60,   10,   39,  210,    9,   20,    2,    1,  192,    9,   66,\n",
       "         54, 3478, 1658,   44,    3,  492,   17, 3639,   44,    3,  180,\n",
       "        540,   19,   18,   44, 5719, 5176,    7,    7,   50,   83,   23,\n",
       "       3625,   31, 1830,    2, 3015,  501, 3625,   21,   39,   31,   38,\n",
       "         48,   42,    3, 4287,   15,    3,   44,   21,  621,    2,  415,\n",
       "         48,   10,  185, 4848,  193,    6,   12,   32, 3971,  576,  542,\n",
       "          3, 5769,    7,    7, 3839,   44,  278,    3, 2249,   38,    3,\n",
       "        240,  805,   18,   21,  702,  156,    7,    7,    1,   17,    6,\n",
       "       1196, 2997,   18,  273,    9,  142,   25, 2015,  137,    3,  120,\n",
       "         51,  573,    3,  420,  800, 2204,   24, 2294,    8,   32, 5770,\n",
       "         18,   24, 4086,    6,  598,  183,    1,    3,  219,    6,   31,\n",
       "       7951,    2, 9498,    2,    8,    1, 3215,  676,  349,   83,   23,\n",
       "       8013,  183,    7,    7,  354], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Inverse Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trapped: buried alive brings us to a resort that has just opened, and is soon to close.<br /><br />We start with a guy in gear blowing up drifts, to avoid the possibility of avalanches. somehow, that doesn't make sense. anyways, he's about to blow away one particularly big one, when he notices the resort is open. despite his best efforts, higher authority tells him his day is over.<br /><br />soon, as everyone expects, an avalanche hits.<br /><br />Look, i'm not gonna reveal any more, all i can say is this was a B-movie designed for the family channel (which i just saw it on, and the fact it had no commercials proves it's a B-movie) anyways, it's a pretty decent film, but it's partially unreal.<br /><br />firsthand, when people are buried by ice and snow, they're buried. not just traced by powder. or, what about a CD for a screwdriver? it's not possible. and finally, what i can't stress enough, is that an explosion cannot stop a avalanche, guaranteed.<br /><br />furthermore, it's worth a rental or a TV viewing, but not owning. 7/10.<br /><br />The movie is rated PG, but maybe it should have received something a little more strong. a boy nearly loses his foot in an elevator, but his leg is cut around the ankle, a guy is toasted by electricity and diesel, and in the weight room, dead people are laying around.<br /><br />enjoy.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"trapped buried alive brings us to a resort that has just opened and is soon to close br br we start with a guy in gear blowing up to avoid the possibility of somehow that doesn't make sense anyways he's about to blow away one particularly big one when he notices the resort is open despite his best efforts higher authority tells him his day is over br br soon as everyone expects an hits br br look i'm not gonna reveal any more all i can say is this was a b movie designed for the family channel which i just saw it on and the fact it had no commercials proves it's a b movie anyways it's a pretty decent film but it's partially unreal br br when people are buried by ice and snow they're buried not just by or what about a cd for a it's not possible and finally what i can't stress enough is that an explosion cannot stop a guaranteed br br furthermore it's worth a rental or a tv viewing but not 7 10 br br the movie is rated pg but maybe it should have received something a little more strong a boy nearly loses his foot in an elevator but his leg is cut around the a guy is by electricity and diesel and in the weight room dead people are laying around br br enjoy\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=16, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=8, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FC/last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 544, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 803s 34ms/step - loss: 0.6637 - acc: 0.5812 - val_loss: 0.6514 - val_acc: 0.6296\n",
      "Epoch 2/3\n",
      " 5056/23750 [=====>........................] - ETA: 9:24 - loss: 0.5165 - acc: 0.7486"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Mis-Classified Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = model.predict(x=x_test_pad[0:1000])\n",
    "y_pred = y_pred.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_true = np.array(y_test[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = incorrect[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = x_test_text[idx]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_true[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "tokens_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embedding = model.get_layer('layer_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding = layer_embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_good = tokenizer.word_index['good']\n",
    "token_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_great = tokenizer.word_index['great']\n",
    "token_great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding[token_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding[token_great]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_bad = tokenizer.word_index['bad']\n",
    "token_horrible = tokenizer.word_index['horrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding[token_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_embedding[token_horrible]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorted Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],\n",
    "                      metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "    \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sorted_words('great', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sorted_words('worst', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
